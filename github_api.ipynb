{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28193808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CONFIG = {\n",
    "    'github_token': os.getenv('GITHUB_TOKEN'),\n",
    "\t'readme_token' : os.getenv('README_TOKEN'),\n",
    "    'host': os.getenv('DB_HOST'),\n",
    "    'user': os.getenv('DB_USER'),\n",
    "    'password': os.getenv('DB_PASSWORD'),\n",
    "    'database': os.getenv('DB_NAME'),\n",
    "    'charset': os.getenv('DB_CHARSET')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "556f6ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub API í† í° ì„¤ì • (GitHubì—ì„œ Personal Access Token ìƒì„± í•„ìš”)\n",
    "github_token = CONFIG['github_token']  # ì—¬ê¸°ì— í† í° ì…ë ¥\n",
    "headers = {'Authorization': f'token {github_token}'}\n",
    "\n",
    "# ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •\n",
    "#keyword = [\"robotics\", \"ROS\", \"robot arm\", \"robot\", \"amr\"]\n",
    "keyword = [\"robotics\"]\n",
    "\n",
    "# GitHub API ì—”ë“œí¬ì¸íŠ¸\n",
    "serch_url = 'https://api.github.com/search/repositories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c18492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‚¤ì›Œë“œ í•´ë‹¹ë˜ëŠ” repo ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def fetch_repos(query, sort='stars', order='desc', per_page=100):\n",
    "    all_repos = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        params = {\n",
    "            'q': f'{query} stars:>=100',  # star ìˆ˜ 100ê°œ ì´ìƒ ì¡°ê±´ ì¶”ê°€\n",
    "            'sort': sort,\n",
    "            'order': order,\n",
    "            'per_page': per_page,\n",
    "            'page': page\n",
    "        }\n",
    "        response = requests.get(serch_url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            repos = response.json()['items']\n",
    "            if not repos:\n",
    "                break\n",
    "            all_repos.extend(repos)\n",
    "            page += 1\n",
    "            # GitHub API rate limitì„ ê³ ë ¤í•´ ìµœëŒ€ 1000ê°œë¡œ ì œí•œ (10í˜ì´ì§€)\n",
    "            if page > 10:\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "    return all_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc06e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo ë©”íƒ€ë°ì´í„° ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜\n",
    "def get_repo_details(repo):\n",
    "    return {\n",
    "\t\t'id': repo['id'],\n",
    "        'full_name': repo['full_name'],\n",
    "        'stargazers_count': repo['stargazers_count'],\n",
    "        'forks_count': repo['forks_count'],\n",
    "        'language': repo['language'],\n",
    "        'created_at': repo['created_at'],\n",
    "        'updated_at': repo['updated_at'],\n",
    "        'open_issues_count': repo['open_issues_count'],\n",
    "        'topics': repo.get('topics', []),\n",
    "        'license': repo['license']['spdx_id'] if repo.get('license') else None,\n",
    "        'owner': repo['owner']['login'],\n",
    "        'description': repo['description'],\n",
    "\t\t'is_fork' : repo['fork'],\n",
    "\t\t'readme' : get_readme(repo[\"owner\"][\"login\"], repo[\"name\"], CONFIG['readme_token'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "066e69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repository readme ì¶”ì¶œ í•¨ìˆ˜\n",
    "def get_readme(owner, repo, token=None):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/readme\"\n",
    "    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n",
    "    if token :\n",
    "        headers[\"Authorization\"] = f\"token {token}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        content = base64.b64decode(data[\"content\"].encode(\"utf-8\")).decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        try:\n",
    "            html = markdown.markdown(content)\n",
    "            text = BeautifulSoup(html, \"html.parser\").get_text(separator=\"\\n\")\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"{owner}/{repo}: Markdown -> Text ë³€í™˜ ì‹¤íŒ¨ {e})\")\n",
    "            return content\n",
    "        \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"{owner}/{repo}: README ì—†ìŒ ({response.status_code})\")\n",
    "        return \"None\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"{owner}/{repo} README ë””ì½”ë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896433e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQLì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë¬´ì‹œí•˜ë„ë¡ ì„¤ì • (Pandasì˜ to_sql ê´€ë ¨ ê²½ê³ ê°€ ëœ° ìˆ˜ ìˆìŒ)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pandas.io.sql\")\n",
    "\n",
    "def save_df_to_sql(df):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ë°ì´í„°í”„ë ˆì„ì„ github_reposì™€ github_readmes í…Œì´ë¸”ì— ë‚˜ëˆ„ì–´ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. ì‚¬ìš©ì ì„¤ì • ---\n",
    "    # ì‚¬ìš©ìì˜ MySQL ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.\n",
    "    DB_USER = CONFIG['user']       # DB ì‚¬ìš©ì ì´ë¦„\n",
    "    DB_PASS = CONFIG['password']   # DB ë¹„ë°€ë²ˆí˜¸\n",
    "    DB_HOST = CONFIG['host']           # DB í˜¸ìŠ¤íŠ¸ (ì˜ˆ: 127.0.0.1)\n",
    "    DB_PORT = \"3306\"                # DB í¬íŠ¸ (ê¸°ë³¸ê°’ 3306)\n",
    "    DB_NAME = CONFIG['database']  # DB ì´ë¦„\n",
    "    \n",
    "    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¬¸ìì—´ ìƒì„± (MySQL + PyMySQL)\n",
    "    connection_string = f\"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}?charset=utf8mb4\"\n",
    "    \n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "    except ImportError:\n",
    "        print(\"ì˜¤ë¥˜: 'pymysql' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. 'pip install pymysql'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. ë°ì´í„° ì „ì²˜ë¦¬ ---\n",
    "    print(\"ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    # ì›ë³¸ ìˆ˜ì •ì„ í”¼í•˜ê¸° ìœ„í•´ ë°ì´í„°í”„ë ˆì„ ë³µì‚¬\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 'created_at', 'updated_at' ì¹¼ëŸ¼ì„ datetime ê°ì²´ë¡œ ë³€í™˜ (ì˜¤ë¥˜ ë¬´ì‹œ)\n",
    "    df_processed['created_at'] = pd.to_datetime(df_processed['created_at'], errors='coerce')\n",
    "    df_processed['updated_at'] = pd.to_datetime(df_processed['updated_at'], errors='coerce')\n",
    "    \n",
    "    # 'topics' ì¹¼ëŸ¼ì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° JSON ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    def serialize_topics(topics_list):\n",
    "        if isinstance(topics_list, (list, dict)):\n",
    "            return json.dumps(topics_list)\n",
    "        elif pd.isna(topics_list):\n",
    "            return None  # NaN, None ë“±ì€ SQL NULLë¡œ ì²˜ë¦¬\n",
    "        return str(topics_list) # ì´ë¯¸ ë¬¸ìì—´ì´ê±°ë‚˜ ë‹¤ë¥¸ íƒ€ì…ì´ë©´ ë¬¸ìì—´ë¡œ\n",
    "        \n",
    "    if 'topics' in df_processed.columns:\n",
    "        df_processed['topics'] = df_processed['topics'].apply(serialize_topics)\n",
    "    \n",
    "    print(\"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.\")\n",
    "\n",
    "    # --- 3. ë°ì´í„° ë¶„ë¦¬ ---\n",
    "    \n",
    "    # 'github_repos' í…Œì´ë¸”ì— í•„ìš”í•œ ì¹¼ëŸ¼ ëª©ë¡\n",
    "    repo_columns = [\n",
    "        'id', 'full_name', 'stargazers_count', 'forks_count', 'language',\n",
    "        'created_at', 'updated_at', 'open_issues_count', 'topics',\n",
    "        'license', 'owner', 'description', 'is_fork'\n",
    "    ]\n",
    "    \n",
    "    # 'github_readmes' í…Œì´ë¸”ì— í•„ìš”í•œ ì¹¼ëŸ¼ ëª©ë¡\n",
    "    readme_columns = ['full_name', 'readme']\n",
    "    \n",
    "    # DataFrameì— í•´ë‹¹ ì¹¼ëŸ¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë¶„ë¦¬\n",
    "    # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¹¼ëŸ¼ì´ ìˆì–´ë„ ì˜¤ë¥˜ ì—†ì´ ì§„í–‰í•˜ë„ë¡ ì²˜ë¦¬\n",
    "    valid_repo_cols = [col for col in repo_columns if col in df_processed.columns]\n",
    "    valid_readme_cols = [col for col in readme_columns if col in df_processed.columns]\n",
    "    \n",
    "    df_repos = df_processed[valid_repo_cols]\n",
    "    df_readmes = df_processed[valid_readme_cols]\n",
    "    \n",
    "    # 'github_readmes'ì˜ 'readme' ì¹¼ëŸ¼ì„ SQL ìŠ¤í‚¤ë§ˆì— ë§ê²Œ 'readme_content'ë¡œ ë³€ê²½\n",
    "    if 'readme' in df_readmes.columns:\n",
    "        df_readmes = df_readmes.rename(columns={'readme': 'readme_content'})\n",
    "\n",
    "    # --- 4. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (íŠ¸ëœì­ì…˜ ì‚¬ìš©) ---\n",
    "    print(\"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    # 'with engine.begin()'ì„ ì‚¬ìš©í•˜ë©´ íŠ¸ëœì­ì…˜ì´ ìë™ìœ¼ë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.\n",
    "    # ë¸”ë¡ ë‚´ ì½”ë“œê°€ ëª¨ë‘ ì„±ê³µí•˜ë©´ ì»¤ë°‹(commit), ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±(rollback)ë©ë‹ˆë‹¤.\n",
    "    try:\n",
    "        with engine.begin() as connection:\n",
    "            \n",
    "            # 1. 'github_repos' í…Œì´ë¸”ì— ì €ì¥ (ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•¨)\n",
    "            print(\"'github_repos' í…Œì´ë¸”ì— ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...\")\n",
    "            df_repos.to_sql(\n",
    "                name='github_repos',\n",
    "                con=connection,\n",
    "                if_exists='append',    # ê¸°ì¡´ í…Œì´ë¸”ì— ë°ì´í„° ì¶”ê°€\n",
    "                index=False,           # pandas ì¸ë±ìŠ¤ëŠ” DBì— ì €ì¥í•˜ì§€ ì•ŠìŒ\n",
    "                chunksize=1000         # ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ë‚˜ëˆ ì„œ ì‚½ì…\n",
    "            )\n",
    "            print(f\"{len(df_repos)}ê°œì˜ ë ˆì½”ë“œë¥¼ 'github_repos'ì— ì €ì¥ ì‹œë„ ì™„ë£Œ.\")\n",
    "\n",
    "            # 2. 'github_readmes' í…Œì´ë¸”ì— ì €ì¥ (ì™¸ë˜ í‚¤ ì œì•½ ì¡°ê±´)\n",
    "            print(\"'github_readmes' í…Œì´ë¸”ì— ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...\")\n",
    "            df_readmes.to_sql(\n",
    "                name='github_readmes',\n",
    "                con=connection,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                chunksize=1000\n",
    "            )\n",
    "            print(f\"{len(df_readmes)}ê°œì˜ ë ˆì½”ë“œë¥¼ 'github_readmes'ì— ì €ì¥ ì‹œë„ ì™„ë£Œ.\")\n",
    "        \n",
    "        print(\"\\nğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (íŠ¸ëœì­ì…˜ ì»¤ë°‹)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ëª¨ë“  ë³€ê²½ì‚¬í•­ì´ ë¡¤ë°±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"ì˜¤ë¥˜ ìƒì„¸ ë‚´ìš©: {e}\")\n",
    "        print(\"íŒ: 'UNIQUE' ì œì•½ ì¡°ê±´(id ë˜ëŠ” full_name) ìœ„ë°˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ë³µëœ ë°ì´í„°ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e45b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching keyword: robotics\n",
      "ì´ 1000ê°œì˜ ê³ ìœ  ë ˆí¬ ìˆ˜ì§‘ ì™„ë£Œ\n",
      "andrewkirillov/AForge.NET: README ì—†ìŒ (404)\n",
      "AgibotTech/agibot_x1_hardware: README ì—†ìŒ (404)\n",
      "MMehrez/MPC-and-MHE-implementation-in-MATLAB-using-Casadi: README ì—†ìŒ (404)\n",
      "EPFLXplore/XRE_LeggedRobot_HW: README ì—†ìŒ (404)\n",
      "CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "all_repos = []\n",
    "for kw in keyword:\n",
    "    print(f\"Searching keyword: {kw}\")\n",
    "    result = fetch_repos(kw)\n",
    "    all_repos.extend(result)\n",
    "\n",
    "# full_name ê¸°ì¤€ ì¤‘ë³µ ì œê±°\n",
    "unique_repos = {repo[\"full_name\"]: repo for repo in all_repos}\n",
    "repos_list = list(unique_repos.values())\n",
    "print(f\"ì´ {len(repos_list)}ê°œì˜ ê³ ìœ  ë ˆí¬ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
    "\n",
    "# DataFrame ìƒì„± ë° CSV ì €ì¥\n",
    "df = pd.DataFrame([get_repo_details(repo) for repo in repos_list])\n",
    "df.to_csv('repos_data.csv', index=False, encoding='utf-8')\n",
    "print(\"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e63403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.\n",
      "ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "'github_repos' í…Œì´ë¸”ì— ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...\n",
      "1000ê°œì˜ ë ˆì½”ë“œë¥¼ 'github_repos'ì— ì €ì¥ ì‹œë„ ì™„ë£Œ.\n",
      "'github_readmes' í…Œì´ë¸”ì— ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...\n",
      "1000ê°œì˜ ë ˆì½”ë“œë¥¼ 'github_readmes'ì— ì €ì¥ ì‹œë„ ì™„ë£Œ.\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (íŠ¸ëœì­ì…˜ ì»¤ë°‹)\n"
     ]
    }
   ],
   "source": [
    "save_df_to_sql(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
